<div align="center">

# Paul Carpenter
### Technical Program Manager Â· AI Systems Architect Â· Platform Builder

[![LinkedIn](https://img.shields.io/badge/LinkedIn-paul--carpenter-0A66C2?style=flat-square&logo=linkedin)](https://linkedin.com/in/paul-carpenter)
[![GitHub](https://img.shields.io/badge/GitHub-pauly7610-181717?style=flat-square&logo=github)](https://github.com/pauly7610)
[![Email](https://img.shields.io/badge/Email-Paul.Carpenter1041@gmail.com-EA4335?style=flat-square&logo=gmail)](mailto:Paul.Carpenter1041@gmail.com)

**$10M+ ARR driven Â· 17+ production systems shipped Â· Enterprise AI with regulatory compliance**

*New York, NY*

</div>

---

## About

I'm a Technical Program Manager who architects and ships production-grade AI systems â€” not just prototypes. With 5+ years leading cross-functional teams across engineering, data science, design, and compliance, I specialize in taking complex AI initiatives from 0â†’1 and scaling them to enterprise deployment.

Currently at **INNOgenius** delivering contextual personalization at scale. Previously drove an **$850M+ platform migration at JPMorgan Chase** and founded a **fintech AI platform that reached $20K+ MRR**.

---

## Featured Projects

### ğŸ§  EvalAI â€” Multi-Agent AI Orchestration & Governance Platform

> *Enterprise-grade platform for deploying, monitoring, auditing, and optimizing multi-agent AI systems with full governance and cost visibility.*

[![Live](https://img.shields.io/badge/Live-v0--ai--evaluation--platform--nu.vercel.app-22C55E?style=flat-square)](https://v0-ai-evaluation-platform-nu.vercel.app/)
[![npm](https://img.shields.io/badge/npm-%40pauly4010%2Fevalai--sdk-CB3837?style=flat-square&logo=npm)](https://www.npmjs.com/package/@pauly4010/evalai-sdk)

**The problem:** Teams deploying multi-agent AI systems have no unified way to visualize workflows, audit decisions, track costs across providers, or enforce compliance policies â€” especially across regulated industries.

**What EvalAI solves:**

| Capability | Detail |
|---|---|
| ğŸ”€ **Multi-Agent Orchestration** | Visual DAG workflows with 6 node types: agent, tool, decision, parallel, human, LLM |
| ğŸ” **Decision Auditing** | Full reasoning chains, confidence scores, and alternative path analysis |
| ğŸ’° **Cost Analytics** | Real-time per-workflow cost tracking across 12 models from 3 providers |
| ğŸ›¡ï¸ **Governance Engine** | Configurable rules with compliance presets: SOC2, GDPR, HIPAA, FINRA |
| â±ï¸ **SLA Management** | Latency, cost, and error rate thresholds with automated violation alerts |
| ğŸ‘¤ **Human-in-the-Loop** | Built-in escalation and approval workflows |
| ğŸ“Š **Agent Benchmarks** | Leaderboards and architecture comparisons |

**How it compares:**

| Feature | EvalAI | LangSmith | PromptLayer | W&B |
|---|:---:|:---:|:---:|:---:|
| Multi-Agent Orchestration | âœ… Full DAG | âš ï¸ Basic | âŒ | âš ï¸ |
| Decision Auditing | âœ… | âš ï¸ Limited | âŒ | âŒ |
| Cost Per Workflow | âœ… Real-time | âŒ | âœ… | âŒ |
| Governance Rules | âœ… + presets | âŒ | âŒ | âŒ |
| Human-in-the-Loop | âœ… Built-in | âŒ | âŒ | âŒ |
| SLA Monitoring | âœ… | âŒ | âŒ | âš ï¸ |
| Compliance Presets | âœ… SOC2/GDPR/HIPAA/FINRA | âŒ | âŒ | âŒ |

**Quick start with the SDK:**
```typescript
import { AIEvalClient, WorkflowTracer } from '@pauly4010/evalai-sdk';

const client = new AIEvalClient({ apiKey: process.env.EVALAI_API_KEY });
const tracer = new WorkflowTracer(client);

await tracer.startWorkflow('Customer Support Pipeline');

await tracer.recordDecision({
  agent: 'RouterAgent',
  type: 'route',
  chosen: 'technical_support',
  alternatives: [{ action: 'billing', confidence: 0.2 }],
  confidence: 85
});

await tracer.recordCost({ provider: 'openai', model: 'gpt-4', inputTokens: 500, outputTokens: 200 });
await tracer.endWorkflow({ resolution: 'Issue resolved' });
```

**Framework integrations:** LangChain (TypeScript) Â· CrewAI (Python) Â· REST API Â· TypeScript SDK

---

### ğŸ¤– Support Intelligence Core (SIC) â€” LLM-Powered Customer Support Platform

> *Modular, production-ready customer support platform with RAG, multi-model AI, agent orchestration, and a 4-layer continuous learning architecture.*

[![GitHub](https://img.shields.io/badge/GitHub-pauly7610%2Fsupport101-181717?style=flat-square&logo=github)](https://github.com/pauly7610/support101)

**The problem:** Enterprise support teams need AI that gets smarter over time â€” not a static chatbot. SIC is a full platform: multi-model RAG, 9 specialized agent blueprints, human-in-the-loop review, voice I/O, and a continuous learning loop that improves with every resolved ticket.

**Architecture:**

```
Frontend (Next.js 15 + Chrome Extension)
         â”‚ Vercel AI SDK streaming / WebSocket
FastAPI Backend (Port 8000)
  â”œâ”€â”€ RAG Engine      â†’ Multi-model: OpenAI, Claude, Gemini, Ollama
  â”œâ”€â”€ Agent Framework â†’ 9 blueprints, tool calling, HITL
  â”œâ”€â”€ Voice I/O       â†’ Whisper STT + OpenAI TTS
  â”œâ”€â”€ A2A Protocol    â†’ JSON-RPC 2.0 agent interoperability
  â””â”€â”€ MCP Server      â†’ 8 tools for IDE/editor integration
         â”‚
Infrastructure: PostgreSQL 16 Â· Redis 7 Â· Pinecone v3 Â· Apache AGE Â· Docker
```

**Continuous Learning â€” 4-layer system (no fine-tuning required):**

| Layer | Technology | Purpose |
|---|---|---|
| Feedback Loop | Pinecone | HITL outcomes â†’ golden paths for future RAG |
| Activity Stream | Redis Streams | Durable event sourcing for all agent actions |
| Activity Graph | Apache AGE | Knowledge graph: Customer â†’ Ticket â†’ Resolution â†’ Agent |
| Playbook Engine | LangGraph | Auto-generated resolution workflows (triggers at 3+ successes) |

> Every layer degrades gracefully: no Redis â†’ in-memory fallback. No AGE â†’ memory. No LangGraph â†’ sequential.

**Key capabilities:**
- **9 agent blueprints:** support, triage, data analyst, code review, QA test, knowledge manager, sentiment monitor, onboarding, compliance auditor
- **Pinecone v3** with bge-reranker-v2-m3 reranking, metadata filtering, namespace isolation, GDPR-compliant deletion
- **Full observability:** OpenTelemetry tracing (Traceloop SDK), Prometheus metrics, Sentry error monitoring with automatic PII scrubbing
- **Multi-tenant RBAC** with JWT auth, per-endpoint rate limiting, and audit logging
- **197 backend tests** across 30 files Â· **Cypress E2E** across 10 spec files

**Tech stack:** FastAPI Â· Next.js 15 Â· React 19 Â· LangChain Â· Pinecone v3 Â· PostgreSQL 16 Â· Redis 7 Â· Apache AGE Â· OpenTelemetry Â· Docker

---

### ğŸ“Š Financial AI Observability Platform â€” Real-Time Anomaly Detection & Compliance

> *Production compliance platform answering three questions regulated AI teams struggle with: Why did the model flag this? Is it still working? Who approved what, and when?*

[![Live](https://img.shields.io/badge/Live-fin--observability--production.up.railway.app-22C55E?style=flat-square)](https://fin-observability-production.up.railway.app)
[![Grafana](https://img.shields.io/badge/Grafana-Live_Dashboard-F46800?style=flat-square&logo=grafana)](https://fin-observability-production.up.railway.app)

**The problem:** AI models in financial services make high-stakes decisions â€” fraud flags, transaction blocks, compliance escalations. Most teams have zero visibility into *why* the model decided, *whether* it's drifting, or *who* approved what override.

**End-to-end decision chain:**

```
Transaction arrives
       â”‚
       â–¼
ML Ensemble (IF + PCA-Autoencoder)    â†’    anomaly score + SHAP explanation
       â”‚
       â–¼
LLM Agent (LangChain)                 â†’    recommendation + reasoning
       â”‚
       â–¼
Human Review (RBAC dashboard)         â†’    final decision + audit log
       â”‚
       â–¼
OpenTelemetry â†’ Grafana Cloud         â†’    every step traced, linked, queryable
```

**Key capabilities:**

| Capability | Detail |
|---|---|
| ğŸ”¬ **Explainability** | SHAP TreeExplainer generates per-prediction feature importance as waterfall charts |
| ğŸ“‰ **Drift Detection** | PSI + KS tests on sliding window; retraining auto-triggers at threshold breach |
| ğŸ” **A/B Testing** | Chi-squared significance testing with hash-based traffic routing |
| ğŸ‘¥ **RBAC** | 4 roles (admin/compliance/analyst/viewer) Â· 25 fine-grained permissions |
| ğŸ“‹ **Audit Trail** | Full chain: inference â†’ agent reasoning â†’ human decision. PII hashed. 7-year retention |
| ğŸ“¡ **Webhook System** | Inbound (10K tx/request) Â· SSE stream Â· outbound callbacks (3x retry + DLQ) Â· scheduled pull |
| ğŸ¤– **MCP Server** | 9 tools â€” connect Claude Desktop, Cursor, or Windsurf directly to the compliance engine |

**Regulatory alignment:** SEC 17a-4 Â· FINRA 4511

**Connect any AI agent via MCP:**
```json
{
  "mcpServers": {
    "fin-observability": {
      "url": "https://fin-observability-production.up.railway.app/mcp",
      "transport": "streamable-http"
    }
  }
}
```

**Available MCP tools:** `check_transaction_compliance` Â· `explain_transaction` Â· `batch_check_compliance` (up to 10K) Â· `analyze_portfolio` Â· `ingest_transactions` Â· `get_compliance_metrics` Â· `list_incidents` Â· `get_drift_status` Â· `get_model_leaderboard`

**Tech stack:** FastAPI Â· scikit-learn Â· ONNX Runtime Â· SHAP Â· LangChain Â· Next.js 16 Â· OpenTelemetry Â· Grafana Cloud Â· PostgreSQL Â· Redis Â· Railway Â· Pulumi

**Test coverage:** 122 backend tests Â· 35 frontend tests Â· GitHub Actions CI

---

## Technical Program Management

Beyond building, I specialize in **coordinating the people and processes** that ship complex AI systems at scale:

- **Program governance** â€” OKRs, risk registers, dependency mapping, stakeholder alignment across engineering, legal, compliance, and design
- **ML lifecycle management** â€” From experimentation frameworks to production release, including bias detection and model interpretability review
- **Enterprise compliance delivery** â€” Coordinating SOC2, GDPR, HIPAA, FINRA controls across regulatory, security, and engineering teams
- **Cross-functional execution** â€” Led $850M+ platform migration at JPMorgan Chase; aligned 4+ engineering squads, legal, compliance, and UX simultaneously

---

## Tech Stack

```
AI/ML          LLMs Â· RAG Â· LangChain Â· Hugging Face Â· SHAP Â· Isolation Forest Â· ONNX Â· OpenAI Â· Anthropic
Backend        FastAPI Â· Go Â· Node.js Â· PostgreSQL Â· Redis Â· SQLAlchemy Â· Alembic
Frontend       Next.js Â· React Â· TypeScript Â· Tailwind CSS Â· Chrome Extensions
DevOps         Docker Â· Kubernetes Â· AWS Â· Railway Â· Vercel Â· GitHub Actions Â· Pulumi
Observability  OpenTelemetry Â· Prometheus Â· Grafana Â· Sentry Â· LangSmith
Compliance     GDPR Â· CCPA Â· SOC2 Â· HIPAA Â· FINRA 4511 Â· SEC 17a-4
PM Tooling     Jira Â· Confluence Â· Figma Â· PRDs Â· Architecture Diagrams
```

---

## Impact

| Metric | Result |
|---|---|
| ARR Growth | $10M+ from deployed ML products |
| Platform Migration | $850M+ rewards platform Â· 25% utilization increase Â· +5 NPS |
| Error Reduction | 95% across 500K+ daily transactions |
| Support Savings | $2M+ annually via NLP resolution accuracy |
| Production Repositories | 17+ shipped 0â†’1 with CI/CD and documentation |
| User Engagement | 40% improvement via contextual AI personalization |

---

<div align="center">

**Open to Senior TPM and AI Product Leadership roles in New York or remote.**

[paul.carpenter1041@gmail.com](mailto:paul.carpenter1041@gmail.com) Â· [LinkedIn](https://linkedin.com/in/paul-carpenter) Â· [GitHub](https://github.com/pauly7610)

</div>

